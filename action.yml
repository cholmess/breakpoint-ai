name: 'BreakPoint Evaluate'
description: 'AI output evaluation & LLM baseline comparison. Prevent bad AI releases before production.'
author: 'Christopher Holmes'
branding:
  icon: 'shield'
  color: 'red'

inputs:
  baseline:
    description: 'Path to baseline JSON file'
    required: true
  candidate:
    description: 'Path to candidate JSON file'
    required: true
  fail_on:
    description: 'Fail the workflow when result equals this (allow = never fail, warn = fail on WARN or BLOCK, block = fail only on BLOCK)'
    required: false
    default: 'warn'
  mode:
    description: 'Evaluation mode (lite or full). Lite: cost, PII, drift. Full: adds output contract, latency, red team, presets.'
    required: false
    default: 'lite'
  extra_args:
    description: 'Optional extra arguments passed to breakpoint evaluate (e.g. --strict --config policy.json)'
    required: false
    default: ''

runs:
  using: 'composite'
  steps:
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install BreakPoint
      shell: bash
      run: pip install breakpoint-ai

    - name: Run BreakPoint Evaluate
      shell: bash
      run: ${{ github.action_path }}/entrypoint.sh
      env:
        INPUT_BASELINE: ${{ inputs.baseline }}
        INPUT_CANDIDATE: ${{ inputs.candidate }}
        INPUT_FAIL_ON: ${{ inputs.fail_on }}
        INPUT_MODE: ${{ inputs.mode }}
        INPUT_EXTRA_ARGS: ${{ inputs.extra_args }}
